import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import HumanMessage, AIMessage
import streamlit as st
from langchain.prompts import PromptTemplate

# Cargar variables de entorno desde el archivo .env
# Esto permite mantener las API keys seguras y fuera del c√≥digo
load_dotenv()

# Configuraci√≥n de la p√°gina de Streamlit
st.set_page_config(page_title="Chatbot B√°sico", page_icon="ü§ñ")
st.title("ü§ñ Chatbot B√°sico con LangChain")
st.markdown("Este es un *chatbot de ejemplo* construido con LangChain + Streamlit. ¬°Escribe tu mensaje abajo para comenzar!")

# Sidebar para configuraci√≥n del chatbot
with st.sidebar:
    st.header("‚öôÔ∏è Configuraci√≥n")
    
    # Control de temperatura: valores m√°s altos = respuestas m√°s creativas
    # valores m√°s bajos = respuestas m√°s deterministas y precisas
    temperature = st.slider("Temperatura", 0.0, 1.0, 0.5, 0.1)
    
    # Selector de modelo (puedes agregar m√°s modelos aqu√≠)
    model_name = st.selectbox("Modelo", ["gemini-2.5-flash", "gemini-pro"])
    
    # Bot√≥n para limpiar el historial de conversaci√≥n
    if st.button("üóëÔ∏è Nueva conversaci√≥n"):
        st.session_state.mensajes = []
        st.rerun()

# Inicializar el modelo de chat con las configuraciones seleccionadas
# La API key se obtiene de forma segura desde las variables de entorno
try:
    chat_model = ChatGoogleGenerativeAI(
        model=model_name,
        temperature=temperature,
        google_api_key=os.getenv("GOOGLE_API_KEY")
    )
except Exception as e:
    st.error(f"‚ùå Error al inicializar el modelo: {str(e)}")
    st.info("Verifica que tu API Key de Google est√© configurada en el archivo .env")
    st.stop()

# Inicializar el estado de sesi√≥n para almacenar el historial de mensajes
# st.session_state persiste los datos entre reruns de Streamlit
if "mensajes" not in st.session_state:
    st.session_state.mensajes = []

# Crear el template de prompt que define el comportamiento del chatbot
prompt_template = PromptTemplate(
    input_variables=["mensaje", "historial"],
    template="""Eres un asistente √∫til y amigable llamado ChatBot Pro. 

Historial de conversaci√≥n:
{historial}

Responde de manera clara y concisa a la siguiente pregunta: {mensaje}"""
)

# Crear cadena usando LCEL (LangChain Expression Language)
# El operador | conecta el prompt template con el modelo de chat
cadena = prompt_template | chat_model

# Mostrar todos los mensajes previos de la conversaci√≥n

for msg in st.session_state.mensajes:
    # Determinar si el mensaje es del asistente o del usuario
    role = "assistant" if isinstance(msg, AIMessage) else "user"
    
    # Renderizar el mensaje con el icono correspondiente
    with st.chat_message(role):
        st.markdown(msg.content)

# Cuadro de entrada de texto para nuevos mensajes
pregunta = st.chat_input("Escribe tu mensaje...")

if pregunta:
    # Mostrar el mensaje del usuario inmediatamente
    with st.chat_message("user"):
        st.markdown(pregunta)

    try:
        # Formatear el historial como texto legible para el modelo
        # Limitamos a los √∫ltimos 10 mensajes para no exceder l√≠mites de tokens
        historial_texto = "\n".join([
            f"{'Usuario' if isinstance(msg, HumanMessage) else 'Asistente'}: {msg.content}"
            for msg in st.session_state.mensajes[-10:]
        ]) if st.session_state.mensajes else "No hay historial previo."
        
        # Generar y mostrar la respuesta del asistente con streaming
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            full_response = ""

            # Streaming: mostrar la respuesta token por token en tiempo real
            for chunk in cadena.stream({
                "mensaje": pregunta,
                "historial": historial_texto
            }):
                full_response += chunk.content
                # El s√≠mbolo ‚ñå simula un cursor parpadeante mientras se escribe
                response_placeholder.markdown(full_response + "‚ñå")
            
            # Mostrar la respuesta completa sin el cursor
            response_placeholder.markdown(full_response)
        
        # Guardar ambos mensajes (usuario y asistente) en el historial
        st.session_state.mensajes.append(HumanMessage(content=pregunta))
        st.session_state.mensajes.append(AIMessage(content=full_response))
        
    except Exception as e:
        st.error(f"‚ùå Error al generar respuesta: {str(e)}")
        st.info("üí° Verifica que tu API Key de Google Gemini est√© configurada correctamente en el archivo .env")